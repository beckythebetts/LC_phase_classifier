{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdFJvuD0nCPm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import os.path, sys\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SdqazW_cJFf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the datasets\n"
      ],
      "metadata": {
        "id": "v5VW1Qxh8U-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06dhcqoSnIob"
      },
      "outputs": [],
      "source": [
        "train_ds = image_dataset_from_directory(\n",
        "    directory = '/content/drive/MyDrive/Train',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='grayscale',\n",
        "    batch_size=16,\n",
        "    image_size=(256, 256)\n",
        ")\n",
        "\n",
        "validation_ds = image_dataset_from_directory(\n",
        "    directory = '/content/drive/MyDrive/Validate',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='grayscale',\n",
        "    batch_size=16,\n",
        "    image_size=(256, 256)\n",
        ")\n",
        "\n",
        "test_ds = image_dataset_from_directory(\n",
        "    directory = '/content/drive/MyDrive/Test',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='grayscale',\n",
        "    batch_size=16,\n",
        "    image_size=(256, 256)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display random selection of traning images with labels"
      ],
      "metadata": {
        "id": "M2DCSfSY8sgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 18))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(16):\n",
        "    ax = plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(tf.reshape(images[i], [256, 256]))\n",
        "    plt.title(train_ds.class_names[tf.argmax(labels[i])])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "MLLHuOww85IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create sharpening layering"
      ],
      "metadata": {
        "id": "8MfoSTmZ8-vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sharpen(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_outputs):\n",
        "        super(Sharpen, self).__init__()\n",
        "        self.num_outputs = num_outputs\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = np.array([[-2, -2, -2],\n",
        "                                [-2, 17, -2],\n",
        "                                [-2, -2, -2]])\n",
        "        self.kernel = tf.expand_dims(self.kernel, -1)\n",
        "        self.kernel = tf.expand_dims(self.kernel, -1)\n",
        "        self.kernel = tf.cast(self.kernel, tf.float32)\n",
        "\n",
        "    def call(self, input_):\n",
        "        return tf.nn.conv2d(input_, self.kernel, strides=1, padding='SAME')"
      ],
      "metadata": {
        "id": "-z3AvVMJ894l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the sequential model"
      ],
      "metadata": {
        "id": "ejvYTzE_9Ln0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhi916eTniBW"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(256, 256, 1)),\n",
        "        layers.Rescaling(1./255),\n",
        "        Sharpen(num_outputs = (256, 256, 1)),\n",
        "        layers.Conv2D(16, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Conv2D(62, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Conv2D(128, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Conv2D(256, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Conv2D(512, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(4, activation='softmax'),\n",
        "\n",
        "    ]\n",
        ")\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define inception model"
      ],
      "metadata": {
        "id": "3yjviZAR-XFZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAxCrF81Sa1n"
      },
      "outputs": [],
      "source": [
        "def inception_module(layer_in, c1, c3_in, c3_out, c5_in, c5_out, mp_out):\n",
        "  conv1 = layers.Conv2D(c1, 1, padding='same', activation='relu')(layer_in)\n",
        "  conv1 = layers.BatchNormalization()(conv1)\n",
        "\n",
        "  conv3 = layers.Conv2D(c3_in, 1, padding='same', activation='relu')(layer_in)\n",
        "  conv3 = layers.BatchNormalization()(conv3)\n",
        "  conv3 = layers.Conv2D(c3_out, 3, padding='same', activation='relu')(conv3)\n",
        "  conv3 = layers.BatchNormalization()(conv3)\n",
        "\n",
        "  conv5 = layers.Conv2D(c5_in, 1, padding='same', activation='relu')(layer_in)\n",
        "  conv5 = layers.BatchNormalization()(conv5)\n",
        "  conv5 = layers.Conv2D(c5_out, 5, padding='same', activation='relu')(conv5)\n",
        "  conv5 = layers.BatchNormalization()(conv5)\n",
        "\n",
        "  pool = layers.MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
        "  pool = layers.Conv2D(mp_out, 1, padding='same', activation='relu')(pool)\n",
        "  pool = layers.BatchNormalization()(pool)\n",
        "\n",
        "  layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
        "\n",
        "  return layer_out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#start_channels\n",
        "N = 2\n",
        "\n",
        "# dropout\n",
        "dr = 0.4\n",
        "\n",
        "# l2 regularisation parameter\n",
        "l2 = 0.001\n",
        "\n",
        "# number of channels\n",
        "c1, c3_in, c3_out, c5_in, c5_out, mp_out = 4*N, N, 2*N, 6*N, 8*N, 2*N\n",
        "\n",
        "inputs = keras.Input(shape=(256, 256, 1))\n",
        "x = Sharpen(num_outputs=(256, 256, 1))(inputs)\n",
        "x = layers.Conv2D(2*N, 7, padding='same', activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D(pool_size = (3,3), padding='same')(x)\n",
        "x = layers.Conv2D(N, 1, padding='same', activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv2D(N, 3, padding='same', activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPooling2D(pool_size = (3,3), padding='same')(x)\n",
        "x = inception_module(x, c1, c3_in, c3_out, c5_in, c5_out, mp_out)\n",
        "x = inception_module(x, c1, c3_in, c3_out, c5_in, c5_out, mp_out)\n",
        "x = inception_module(x, c1, c3_in, c3_out, c5_in, c5_out, mp_out)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(8*N, activation='relu', kernel_regularizer=regularizers.l2(l2))(x)\n",
        "x = layers.Dropout(dr)(x)\n",
        "x = layers.Dense(4*N, activation='relu', kernel_regularizer=regularizers.l2(l2))(x)\n",
        "x = layers.Dropout(dr)(x)\n",
        "outputs = layers.Dense(4, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "wKyl2P4Q-iGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and train model"
      ],
      "metadata": {
        "id": "jwn3rSH69g_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y0BsjZrjoLlg"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    optimizer=keras.optimizers.Adam(lr=1e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "history = model.fit(train_ds, epochs=100, verbose=1, callbacks=[model_checkpoint_callback], validation_data=validation_ds)\n",
        "\n",
        "model.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot accuracy over each epoch\n"
      ],
      "metadata": {
        "id": "WyB5nC3z9n1K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZLkaauyk8AEq"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validate'])\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot loss over each epoch"
      ],
      "metadata": {
        "id": "MyEZ0A-a9sC6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WT13d834oTyp"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validate'])\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model on test and validation datasets"
      ],
      "metadata": {
        "id": "KI1uAANN9xzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MFRsi24ronmn"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_ds, verbose=1)\n",
        "model.evaluate(validation_ds, verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot confusion matrix"
      ],
      "metadata": {
        "id": "D3g4h9Vb-B1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vmPavYNWa0O"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "predictions_initial = model.predict(test_ds)\n",
        "predictions = np.array([])\n",
        "labels =  np.array([])\n",
        "\n",
        "test_ds.reset()\n",
        "wrong_predictions_array = np.array([])\n",
        "for i in range(test_ds.__len__()):\n",
        "  image = test_ds\n",
        "  labels = np.concatenate([labels, np.argmax(test_ds.next()[1], axis=-1)])\n",
        "  predictions = np.concatenate([predictions, [np.argmax(predictions_initial[i], axis=0)]])\n",
        "\n",
        "cm = confusion_matrix(labels, predictions)\n",
        "\n",
        "fig, ax  = plt.subplots(figsize=(7.5, 7.5))\n",
        "ax.matshow(cm, cmap='Blues')\n",
        "tick_labels = [''] + classes\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
        "\n",
        "ax.set_xticklabels(tick_labels)\n",
        "ax.set_yticklabels(tick_labels)\n",
        "plt.xlabel('Predictions', fontsize=18)\n",
        "plt.ylabel('Actuals', fontsize=18)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}